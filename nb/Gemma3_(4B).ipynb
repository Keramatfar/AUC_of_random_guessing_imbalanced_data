{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keramatfar/AUC_of_random_guessing_imbalanced_data/blob/main/nb/Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4BPv5hE9Ex"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + â­ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvqi7A7-E9Ez"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYhMnI2uE9Ez"
      },
      "source": [
        "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvrXvYzE9E0"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O7oYxlcME9E0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7kbR7CyzE9E1"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "-Xbb0cuLzwgf",
        "outputId": "5479fba7-513e-46b2-eb7d-ba541d485c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 32896.5 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b7006fa3493f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m ] # More models at https://huggingface.co/unsloth\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m model, tokenizer = FastModel.from_pretrained(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/gemma-3-4b-it\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Choose any for long context!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, *args, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mauto_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForVision2Seq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_vlm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         model, tokenizer = FastBaseModel.from_pretrained(\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/vision.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, trust_remote_code, model_types, tokenizer_name, auto_model, use_gradient_checkpointing, supports_sdpa, whisper_language, whisper_task, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_forced_float32\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         model = auto_model.from_pretrained(\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Gemma-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Hello!<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Hey there!<end_of_turn>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkq4RvEq7FQr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reoBXmAn7HlN"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_data_formats\n",
        "dataset = standardize_data_formats(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5Sx9In7vHi"
      },
      "source": [
        "Let's see how row 100 looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzE1OEXi7s3P"
      },
      "outputs": [],
      "source": [
        "dataset[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xs0LXio7rfd"
      },
      "source": [
        "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ahE8Ys37JDJ"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "Let's see how the chat template did! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "dataset[100][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are an AI tasked with describing an twitter account based on previous data have been used for our training. Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ú©Ø§Ø±Ø¨Ø± ØªÙˆØ¦ÛŒØªØ± Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ùˆ Ú†Ù†Ø¯ Ù¾Ø³Øª Ø§Ùˆ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒ Ø¢ÛŒØ¯.: {'title': 'AbdiMedia - Abdollah Abdi', 'bio': 'Independent Journalist, Analyst and Historian. Founder of AbdiMedia.', 'user_name': 'abdolah_abdi', 'posts': [{'text': '\\nğŸ”´\\n Ø§Ù…Ø´Ø¨ \\n#Ø²Ù†Ø¯Ù‡\\n / Ø§ÛŒØ±Ø§Ù† Ùˆ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¨Ø± Ù„Ø¨Ù‡ ØªÛŒØº! \\nØªÙˆØ§ÙÙ‚\\n ÛŒØ§ ØªÙ‚Ø§Ø¨Ù„Ø› Ú©Ø¯Ø§Ù… Ù†Ø²Ø¯ÛŒÚ©ØªØ±ØŸ\\nğŸ”¸\\nØ¨Ø§ Ø­Ø¶ÙˆØ± Ø¯Ú©ØªØ± ØµØ§Ù„Ø­ Ø§Ø³Ú©Ù†Ø¯Ø±ÛŒØŒ Ø§Ø³ØªØ§Ø¯ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ùˆ ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù…Ø³Ø§Ø¦Ù„ Ø³ÛŒØ§Ø³ÛŒ Ùˆ Ø¹Ø¶Ùˆ Ø´ÙˆØ±Ø§ÛŒ Ù…Ø±Ú©Ø²ÛŒ Ø´Ø±ÛŒØ§Ù† (Ø´Ø¨Ú©Ù‡ Ø±Ø§Ù‡Ø¨Ø±Ø¯ÛŒ ÛŒØ§Ø±Ø§Ù† Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ø³Ù„Ø§Ù…ÛŒ)\\nâ°\\n Ø³Ø§Ø¹Øª Û²Û²:Û°Û° Ø¨Ù‡ ÙˆÙ‚Øª Ø§ÛŒØ±Ø§Ù†\\nğŸ“¡\\n Ù¾Ø®Ø´ \\n#Ø²Ù†Ø¯Ù‡\\n Ù‡Ù…Ø²Ù…Ø§Ù† Ø§Ø² Ú©Ø§Ù†Ø§Ù„\\u200c ÛŒÙˆØªÛŒÙˆØ¨ØŒ ØµÙØ­Ù‡ ØªÙˆÛŒÛŒØªØ± Ùˆ', 'retweet_count': 0, 'like_count': 3, 'tweet_categories': ['politic']}, {'text': '\\nğŸ”´\\n Ø±ÛŒØ²Ø´ Û²Û³ Ù‡Ø²Ø§Ø± ÙˆØ§Ø­Ø¯ÛŒ Ø´Ø§Ø®Øµ Ø¨ÙˆØ±Ø³\\nğŸ’¢\\nØ´Ø§Ø®Øµ Ú©Ù„ Ø¨ÙˆØ±Ø³ Ø¨Ø§ Ø±ÛŒØ²Ø´ Û²Û³ Ù‡Ø²Ø§Ø± ÙˆØ§Ø­Ø¯ÛŒ Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù…Ø±ÙˆØ² Ø¨Ù‡ Û³ Ù…ÛŒÙ„ÛŒÙˆÙ† Ùˆ Û°Û¹Û² Ù‡Ø²Ø§Ø± ÙˆØ§Ø­Ø¯ Ø±Ø³ÛŒØ¯.', 'retweet_count': 0, 'like_count': 13, 'tweet_categories': ['economic']}, {'text': '\\nğŸ”´\\n Ù¾Ø²Ø´Ú©ÛŒØ§Ù†: Ù…Ø§ Ø§Ù„Ø§Ù† Ø¨Ú†Ù‡\\u200cÙ‡Ø§ÛŒÛŒ ØªØ±Ø¨ÛŒØª Ù…ÛŒ\\u200cÚ©Ù†ÛŒÙ… Ú©Ù‡ ÙÚ©Ø±Ø´Ø§Ù† Ø¨Ù‡ Ø®Ø§Ø±Ø¬ Ø§Ø³Øª. Ù‡Ù†Ø± Ø§ÛŒÙ† Ù†ÛŒØ³Øª Ú©Ù‡ Ù†Ø®Ø¨Ù‡ ØªØ±Ø¨ÛŒØª Ú©Ù†ÛŒÙ… Ùˆ Ø¨Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¨ÙØ±Ø³ØªÛŒÙ….', 'retweet_count': 0, 'like_count': 11, 'tweet_categories': ['politic', 'social']}, {'text': '\\nğŸ¤\\n \\n#Ø¨Ø´Ù†ÙˆÛŒØ¯\\n | Ù¾Ø³ Ø§Ø² Ø³Ø§Ù„\\u200cÙ‡Ø§ Ø³Ú©ÙˆØª! Ø§ÙØ´Ø§Ú¯Ø±ÛŒ ÛŒÚ© Ù…Ù‚Ø§Ù… Ø§Ø±Ø´Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ\\nğŸ”¸\\nÚ¯ÙØªÚ¯Ùˆ Ø¨Ø§ Ø´ÛŒØ® Ù…ØµØ·ÙÛŒ Ø³Ù†Ø§ÛŒÛŒ ÙØ±ØŒ Ù‚Ø§Ø¦Ù… Ù…Ù‚Ø§Ù… ÙˆÙ‚Øª Ù…Ø¹Ø§ÙˆÙ†Øª ÙˆÛŒÚ˜Ù‡ Ùˆ Ù…Ø¹Ø§ÙˆÙ†Øª Ø­ÙØ§Ø¸Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ²Ø§Ø±Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ Ù‚Ø§Ø¦Ù… Ù…Ù‚Ø§Ù… Ù…Ø­Ø³Ù†ÛŒ Ø§Ú˜ÛŒÙ‡ Ø¯Ø± Ø¯Ø§Ø¯Ø³Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ù‡ Ø±ÙˆØ­Ø§Ù†ÛŒØª\\nğŸ“»\\n Ø¨Ø¯ÙˆÙ† ÙÛŒÙ„ØªØ± Ø¯Ø± Ú©Ø³Øª\\u200cØ¨Ø§Ú©Ø³ Ø¨Ø´Ù†ÙˆÛŒØ¯\\nğŸ‘‡\\n\\nğŸ”½\\n \\nhttps://t.co/fBIQfDKSCM', 'retweet_count': 0, 'like_count': 5, 'tweet_categories': ['security_defense']}, {'text': '\\nğŸ”´\\n Ø§Ù…Ø§Ù†ÛŒ: Ø´Ù‡Ø±ÙˆÙ†Ø¯Ø§Ù† ØªÙ‡Ø±Ø§Ù† Ø¨Ø§ Ø²Ø¨Ø§Ù„Ù‡\\u200cÙ‡Ø§ Ù‚Ø¶Ø§ÙˆØª\\u200cÙ…Ø§Ù† Ù…ÛŒ\\u200cÚ©Ù†Ù†Ø¯ØŒ Ù†Ù‡ Ø¨Ø§ Ø´Ø¹Ø§Ø±Ù‡Ø§!', 'retweet_count': 0, 'like_count': 15, 'tweet_categories': ['politic', 'social']}, {'text': '\\nğŸ”´\\n Ø±ÛŒÛŒØ³ Ù¾Ù„ÛŒØ³ ÙØªØ§ \\nÙØ±Ø§Ø¬Ø§\\n: ÙØ±ÙˆØ´ Ù…Ø¯Ø§Ø±Ú© Ùˆ ÙˆØ§Ù… ÙÙˆØ±ÛŒ Ú©Ù„Ø§Ù‡Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø³Øª/ Ù…Ø±Ø¯Ù… Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ ØµÙØ­Ø§ØªÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø±Ø§ ØªØ¨Ù„ÛŒØº Ù…ÛŒ Ú©Ù†Ù†Ø¯ØŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø´Ù…Ø§Ø±Ù‡ Û°Û¹Û¶Û³Û¸Û° Ø¨Ù‡ Ù¾Ù„ÛŒØ³ ÙØªØ§ Ø§Ø·Ù„Ø§Ø¹ Ø¯Ù‡Ù†Ø¯', 'retweet_count': 3, 'like_count': 9, 'tweet_categories': ['security_defense']}, {'text': '\\nğŸ”´\\n Ø¢ÛŒØª\\u200cØ§Ù„Ù„Ù‡ Ø®Ø§Ù…Ù†Ù‡\\u200cØ§ÛŒ: Ù…Ø´Ú©Ù„Ø§Øª Ù¾ÛŒØ´ Ø¢Ù…Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÚ¯Ø§Ù‡\\u200cÙ‡Ø§ Ø¯Ø± Ø­Ø§Ø¯Ø«Ù‡ Ø¨Ù†Ø¯Ø±Ø´Ù‡ÛŒØ¯ Ø±Ø¬Ø§ÛŒÛŒØŒ Ø§Ù†\\u200cØ´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø¨Ø§ ÙÙˆØ±ÛŒØªØŒ Ù‚Ø¯Ø±Øª Ùˆ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø¯Ø³ØªÚ¯Ø§Ù‡\\u200cÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø³Ø±Ø­Ø§Ù„ Ùˆ ØªÙˆØ§Ù†Ø§ Ùˆ Ø¬ÙˆØ§Ù† Ù…Ø§ Ø¬Ø¨Ø±Ø§Ù† Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯', 'retweet_count': 0, 'like_count': 8, 'tweet_categories': ['politic', 'religion']}, {'text': '\\nğŸ”´\\n Ø¯Ù…Ø§ÛŒ Ø§Ù†ÙØ¬Ø§Ø± Ø§Ø³Ú©Ù„Ù‡ Û±Û´Û°Û° Ø¯Ø±Ø¬Ù‡ Ø¨ÙˆØ¯/ Ø¢Ø¨ Ø±ÙˆÛŒ Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ Ø¨Ø®Ø§Ø± Ù…ÛŒ Ø´Ø¯\\nğŸ’¢\\n Ø±ÙˆØ§ÛŒØª Ù…Ø¯ÛŒØ± Ø¹Ø§Ù…Ù„ Ø¢ØªØ´ Ù†Ø´Ø§Ù†ÛŒ ØªÙ‡Ø±Ø§Ù† Ø§Ø² Ø­Ø§Ø¯Ø«Ù‡ Ø¨Ù†Ø¯Ø± Ø´Ù‡ÛŒØ¯ Ø±Ø¬Ø§ÛŒÛŒ:\\nğŸ“Œ\\n Ø¯Ù…Ø§ Ø¯Ø± Ù…Ø­Ù„ Ø­Ø±ÛŒÙ‚ Û±Û´Û°Û° Ø¯Ø±Ø¬Ù‡ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§ Ø¢Ø¨ Ù†Ù…ÛŒ Ø´Ø¯ Ø¢ØªØ´ Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ Ø±Ø§ Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯\\nğŸ“Œ\\n Ø¢Ø¨ Ù‡Ù…Ø§Ù† Ù„Ø­Ø¸Ù‡ Ø¨Ø®Ø§Ø± Ù…ÛŒ Ø´Ø¯.\\nğŸ“Œ\\n Ø­Ø±ÛŒÙ‚ Ù‡Ø§ Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ø¯Ø«Ù‡ØŒ Ø¯Ø± Ú©Ø§Ù†ØªÛŒØ±Ù‡Ø§ÛŒ Ø¨Ø³ØªÙ‡ Ø§ØªÙØ§Ù‚', 'retweet_count': 0, 'like_count': 11, 'tweet_categories': ['science_tech_health']}, {'text': '\\nğŸ”´\\n Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ø´Ù‡Ø±Ø¯Ø§Ø± Ø¨Ø§ ØºÛŒØ±Øª Ø´ÛŒØ±Ø§Ø²\\nğŸ”¸\\nØ§ÛŒÙ† ÙˆÛŒØ¯ÛŒÙˆ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÚ© Ù…Ø§Ø¯Ø± Ø¯Ø³ØªÙØ±ÙˆØ´ Ø¯Ø± Ø´ÛŒØ±Ø§Ø² Ø§Ø³Øª.', 'retweet_count': 5, 'like_count': 49, 'tweet_categories': ['security_defense']}, {'text': '\\nğŸ”´\\n Ø³Ø±Ù„Ø´Ú©Ø± Ø³Ù„Ø§Ù…ÛŒ: Ù…Ø§ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ±ÙˆØ²ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡\\u200cØ§ÛŒÙ…/ Ø³Ù¾Ø§Ù‡ Ø¯Ø± Ø¢Ù…ÙˆØ²Ù‡\\u200cÙ‡Ø§ÛŒ Ø®ÙˆØ¯ ÛŒÚ© Ú¯Ø§Ù… Ø¹Ù‚Ø¨ \\u200cÙ†Ø´ÛŒÙ†ÛŒ ØªØ¹Ø±ÛŒÙ Ù†Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª\\nğŸ’¢\\n ÙØ±Ù…Ø§Ù†Ø¯Ù‡ Ú©Ù„ Ø³Ù¾Ø§Ù‡ Ø¯Ø± Ù…Ø±Ø§Ø³Ù… Ø¨Ø²Ø±Ú¯Ø¯Ø§Ø´Øª Ù…Ù‚Ø§Ù… Ù…Ø¹Ù„Ù… Ùˆ Ù‡ÙØªÙ‡ Ø¹Ù‚ÛŒØ¯ØªÛŒ Ø³ÛŒØ§Ø³ÛŒ:\\nğŸ“Œ\\n Ù‡Ø±Ú¯Ø² Ù…ÙˆÙ…Ù†ÛŒÙ† Ù†Ø¨Ø§ÛŒØ¯ Ø´Ú©Ø³Øª Ø¨Ø®ÙˆØ±Ù†Ø¯Ø› Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ù†Øª Ø§Ù„Ù‡ÛŒ Ø§Ú¯Ø± Ø­Ú©Ù…Øª Ù¾ÛŒØ±ÙˆØ²ÛŒ Ø±Ø§ Ø¨ÛŒØ§Ø¨Ù†Ø¯ Ùˆ Ø¨Ø± Ø³Ù†Øª\\u200cÙ‡Ø§ Ùˆ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù‡ÛŒ', 'retweet_count': 3, 'like_count': 10, 'tweet_categories': ['politic', 'religion', 'security_defense']}]} . Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø± Ø­Ø¯ ÛŒÚ© Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ÛŒÙ† Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ ØªÙˆØµÛŒÙ Ú©Ù†.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"gemma-3\")\n",
        "\n",
        "# Suppose you have a list of prompts\n",
        "prompts = [prompt]*1  # Replace with your actual list of prompts\n",
        "prompt = 'what is the language of this text: '\n",
        "# Convert prompts to message format\n",
        "messages_list = [[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": p}]}] for p in prompts]\n",
        "\n",
        "# Apply chat templates and tokenize\n",
        "texts = [\n",
        "    tokenizer.apply_chat_template(m, add_generation_prompt=True)\n",
        "    for m in messages_list\n",
        "]\n",
        "tokenized = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "# Generate outputs\n",
        "outputs = model.generate(\n",
        "    **tokenized,\n",
        "    max_new_tokens=250,\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        "    top_k=64,\n",
        ")\n",
        "\n",
        "# Decode\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "NyOHIUI3Meht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "re.findall('<start_of_turn>model\\n(.*)', tokenizer.batch_decode(outputs)[0])[0]"
      ],
      "metadata": {
        "id": "UQV_AkyIJ15D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastModel\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3?\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save finetune!\n",
        "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6O48DbNIAr0"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-CiKPrIFG0"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save to GGUF\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q974YEVPI7JS"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgcJIhJ0I_es"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
        "        token = \"hf_...\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQR6x5YNE9FC"
      },
      "source": [
        "Now, use the `gemma-3-finetune.gguf` file or `gemma-3-finetune-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + â­ï¸ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­ï¸\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}